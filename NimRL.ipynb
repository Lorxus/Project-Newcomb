{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPy1JQpcyeLagJGzp0+24Hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lorxus/Project-Newcomb/blob/main/NimRL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZ9qQwMZucB1"
      },
      "outputs": [],
      "source": [
        "# defines <=4-column Nim\n",
        "import numpy as np\n",
        "import random as rand\n",
        "\n",
        "# defines game\n",
        "class Nim:\n",
        "    def __init__(self, board = [0, 0, 0, 0], firstplayerhuman = False, secondplayerhuman = False):  # 4 colors of berry, why not\n",
        "        self.board = board\n",
        "        self.startboard = board\n",
        "\n",
        "        self.players = ['1', '2']\n",
        "\n",
        "        tempplayers = []\n",
        "        if firstplayerhuman:\n",
        "            tempplayers.append('1')\n",
        "        if secondplayerhuman:\n",
        "            tempplayers.append('2')\n",
        "        self.humans = tempplayers\n",
        "\n",
        "        self.current_player = '1'\n",
        "        self.winner = None\n",
        "        self.game_over = False\n",
        "        self.reward: float | None = None\n",
        "\n",
        "    def reset(self):\n",
        "        self.board = self.startboard\n",
        "        self.current_player = '1'\n",
        "        self.winner = None\n",
        "        self.game_over = False\n",
        "\n",
        "    def get_available_moves(self):  # pick a pile and eat any number of the same color\n",
        "        moves = []\n",
        "        for i in range(4):\n",
        "            for j in range(self.board[i-1]):\n",
        "              moves.append((i, j))\n",
        "        return moves\n",
        "\n",
        "    def make_move(self, move):  # this isn't going to be how human players submit moves, just backend\n",
        "        if move == None or self.currentplayer in self.humans:  # we'll use a dummy move for the human player...\n",
        "            move = self.prompthumanmove()  # ...then overwrite it\n",
        "\n",
        "        self.board[move[0]] = self.board[move[0]] - self.board[move[1]]  # the human player can't submit illegal moves and the CPU never will\n",
        "\n",
        "        if self.check_winner() is not None:  # if the game isn't over, switch players\n",
        "            self.switch_player()\n",
        "            self.print_board()\n",
        "            return True\n",
        "        else:  # if it is, then say so\n",
        "            print('Game over! Winner: Player', self.current_player)\n",
        "            self.score()\n",
        "\n",
        "    def switch_player(self):\n",
        "        if self.current_player == self.players[0]:\n",
        "          self.current_player = self.players[1]\n",
        "        else:\n",
        "          self.current_player = self.players[0]\n",
        "\n",
        "    def check_winner(self): # if there's no berries left at the end of your turn you win\n",
        "        if sum(self.board) > 0:\n",
        "          return None\n",
        "        elif self.current_player == '1':\n",
        "          return '1'\n",
        "        else:\n",
        "          return '2'\n",
        "\n",
        "    def get_board(self):\n",
        "        return self.board\n",
        "\n",
        "    def print_board(self):\n",
        "        humanity = self.currentplayer in self.humans\n",
        "        nottedness = 'is'\n",
        "        if not humanity:\n",
        "            nottedness = 'is not'\n",
        "        print('Current player is: Player', self.currentplayer, 'who', nottedness, 'a human player.', 'The board state is:', self.board[0], 'red,', self.board[1], 'yellow,', self.board[2], 'green,', self.board[3], 'blue.')\n",
        "        return True\n",
        "\n",
        "    def score(self):\n",
        "        if self.game_over == False:\n",
        "          self.reward = None\n",
        "        elif self.winner == '1':\n",
        "          self.reward = 1\n",
        "        elif self.winner == '2':\n",
        "          self.reward = -1\n",
        "\n",
        "    def prompthumanmove(self):\n",
        "        print('Enter pile number (0-3).')\n",
        "        pilenumber = int(input())\n",
        "        while self.board[pilenumber] < 1:\n",
        "            print('Pile is empty - pick a different one?')\n",
        "            pilenumber = int(input)\n",
        "\n",
        "        print('Pile', pilenumber, 'has', self.board[pilenumber], 'berries; please enter that number or fewer to make your move.')\n",
        "        berrynumber = int(input)\n",
        "        while berrynumber > self.board[pilenumber] or berrynumber < 1:\n",
        "            print('Bad number - try that again?')\n",
        "            berrynumber = int(input)\n",
        "\n",
        "        mymove = [pilenumber, berrynumber]\n",
        "        return mymove\n",
        "\n",
        "    def testcase_factory(param: int):  # creates a random board with max berries param per pile and returns the Nim game that has that board as its start state\n",
        "        randboard = []\n",
        "        for i in range(4):\n",
        "            temprand = rand.randint(0, param)\n",
        "            randboard.append(temprand)\n",
        "\n",
        "        newgame = Nim(randboard)\n",
        "        return newgame\n",
        "\n",
        "# need to code up a playable test case for two human players..."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# implement Q-learning\n",
        "# this is the part that actually invokes Q-learning/RL\n",
        "class QLearningAgent:\n",
        "  def __init__(self, alpha, epsilon, discount_factor):\n",
        "    self.Q = {}  # (state, action). what have we tried so far?\n",
        "    self.payoff = {}  # ((state, action), payoff). what did it result in?/what is it predicted/expected to result it?\n",
        "    self.alpha = alpha  # learning rate\n",
        "    self.epsilon = epsilon  # exploration rate\n",
        "    self.gamma = discount_factor  # time-discount factor\n",
        "\n",
        "  # build up a dictionary of state-action pairs with associated node values (Q-values) - this part will need to interface very directly with any IB stuff I do\n",
        "  def get_Q_value(self, state, action):\n",
        "    if (state, action) not in self.Q:\n",
        "      self.Q[(state, action)] = 0.0\n",
        "    return self.Q[(state, action)]\n",
        "\n",
        "  # update the Q-values labelling the entries based on difference between expected and actual rewards\n",
        "  # this will also need to be heavily adapted for IB, if/when I get there\n",
        "  def update_Q_value(self, state, action, next_state):\n",
        "    oldQ = self.get_Q_value(state, action)\n",
        "\n",
        "    Q_values = [self.get_Q_value(next_state, action) for action in Nim(state).get_available_moves()]\n",
        "    max_Q = max(Q_values)\n",
        "\n",
        "    self.Q[(state, action)] = oldQ + self.alpha * ((self.gamma *  max_Q) - oldQ)  # immediate reward for Nim is always 0\n",
        "    # Q(S_t, A_t) = Q(S_t, A_t) + a(R_t+1 + g max_act Q(S_t+1, A_t+1) - Q(S_t, A_t))\n",
        "\n",
        "  # choose an action\n",
        "  # with p=eps, pick a random new one - this might look like it'll cause problems with IB but actually there's a specific way to get around that.\n",
        "  # with p = 1-eps, pick an action with the greatest expected node value (Q-value).\n",
        "  def choose_action(self, state):\n",
        "    if rand.uniform(0, 1) < self.epsilon:\n",
        "      return rand.choice(Nim(state).get_available_moves())\n",
        "    else:\n",
        "      Q_values = [self.get_Q_value(state, action) for action in Nim(state).get_available_moves()]\n",
        "      max_Q = max(Q_values)\n",
        "      if Q_values.count(max_Q) > 1:\n",
        "        best_moves = [i for i in range(len(Nim(state).get_available_moves())) if Q_values[i] == max_Q]\n",
        "        i = rand.choice(best_moves)\n",
        "      else:\n",
        "        i = Q_values.index(max_Q)\n",
        "      return Nim(state).get_available_moves()[i]\n",
        "\n",
        "\n",
        "  # EVERYTHING BELOW IS INCOMPLETE.\n",
        "  # I will likely need to redo a lot of it!\n",
        "  # trains creates a RL bot with specified learning rate alpha, exploration rate eps, and time-discount factor discount_factor to train for num_episodes games.\n",
        "  def train(self, num_episodes):\n",
        "    agent = QLearningAgent(self.alpha, self.epsilon, self.gamma)\n",
        "    gameslate = []\n",
        "    for i in range(num_episodes):\n",
        "        randomboard = []\n",
        "        for j in range(4):\n",
        "            randomboard.append(rand.randint(0, 15))\n",
        "        game = Nim(randomboard)\n",
        "        gameslate.append(game)\n",
        "\n",
        "    for i in range(num_episodes):\n",
        "        game = gameslate[i]\n",
        "        state = game.board\n",
        "        while not game.game_over:\n",
        "            action = agent.choose_action(state)\n",
        "            next_state = game.make_move(action)\n",
        "            agent.update_Q_value(game.board, action, next_state)\n",
        "            game = next_state\n",
        "    return agent\n",
        "\n",
        "  # tests the resulting trained agent\n",
        "  # this part is confused. it probably needs to pull a slate of Nim games?\n",
        "  def testwinrate(self, num_games):\n",
        "    num_wins = 0\n",
        "    gameslate = []\n",
        "    for i in range(num_games):\n",
        "        randomboard = []\n",
        "        for j in range(4):\n",
        "            randomboard.append(rand.randint(0, 15))\n",
        "        game = Nim(randomboard)\n",
        "        gameslate.append(game)\n",
        "\n",
        "    for i in range(num_games):\n",
        "        game = gameslate[i]\n",
        "        state = game.board\n",
        "        while not game.game_over:\n",
        "            if game.current_player == '1':\n",
        "                action = agent.choose_action(state)\n",
        "            else:\n",
        "                action = rand.choice(game.get_available_moves())\n",
        "            state = game.make_move(action)\n",
        "\n",
        "        score = game.score()\n",
        "        if score == 1:\n",
        "            num_wins += 1\n",
        "    return num_wins / num_games * 100\n",
        "\n",
        "# creates a test Nim game with specified piles\n",
        "boardstate = [2, 1, 0, 0]\n",
        "game = Nim(boardstate)\n",
        "agent = QLearningAgent(alpha=0.5, epsilon=0.1, discount_factor=1.0)\n",
        "\n",
        "# train the Q-learning agent\n",
        "agent = agent.train(num_episodes=100000)\n",
        "\n",
        "# test the Q-learning agent\n",
        "win_percentage = agent.testwinrate(num_games=1000)\n",
        "print(\"Win percentage: {:.2f}%\".format(win_percentage))"
      ],
      "metadata": {
        "id": "v1axJ9-w1aWk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "outputId": "5c9cae30-4683-4da7-90c4-708775165706"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unhashable type: 'list'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-3f935a9b78ea>\u001b[0m in \u001b[0;36m<cell line: 100>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;31m# train the Q-learning agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;31m# test the Q-learning agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-3f935a9b78ea>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_episodes)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_over\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_Q_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-3f935a9b78ea>\u001b[0m in \u001b[0;36mchoose_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mrand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_available_moves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m       \u001b[0mQ_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_Q_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mNim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_available_moves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m       \u001b[0mmax_Q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mQ_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_Q\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-3f935a9b78ea>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mrand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_available_moves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m       \u001b[0mQ_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_Q_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mNim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_available_moves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m       \u001b[0mmax_Q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mQ_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_Q\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-3f935a9b78ea>\u001b[0m in \u001b[0;36mget_Q_value\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;31m# build up a dictionary of state-action pairs with associated node values (Q-values) - this part will need to interface very directly with any IB stuff I do\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_Q_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
          ]
        }
      ]
    }
  ]
}